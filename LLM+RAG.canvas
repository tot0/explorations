{
	"nodes":[
		{"type":"group","id":"12ec1dd5c1dd11aa","x":-1400,"y":-880,"width":1842,"height":469,"label":"Simple LLM Flow"},
		{"type":"link","url":"https://lh5.googleusercontent.com/1mMKU2jr3BSGshMcKhpNwdn4n7zV5kUW2a_Jp5xt2vXpw99WSixVOA8WVMjVJ9vGc2eyCqdf5SMm-PbFPrECr_5iDyiOChVj1wtJulaJ2oG6Qe5uJNZ1ViBsdEr53A3o1ckDIpZwy8x_clZR865Db10","id":"2821f41f379d49a5","x":-1400,"y":480,"width":1623,"height":540},
		{"type":"text","text":"## [RAG](https://arxiv.org/abs/2005.11401?ref=matt-boegner) / [RETRO](https://arxiv.org/abs/2112.04426?ref=matt-boegner) / [REALM](https://arxiv.org/abs/2002.08909?ref=matt-boegner)","id":"9e17d465261052bd","x":-843,"y":320,"width":509,"height":50},
		{"type":"link","url":"https://mattboegner.com/content/images/2023/01/Screen-Shot-2023-01-31-at-12.44.42-AM.png","id":"930956982131d1e8","x":-1057,"y":1439,"width":938,"height":186},
		{"type":"text","text":"Lots of experimentation done by 'Prompt Engineers' to figure out the right prompts for different tasks.","id":"acfd8dcba8c03748","x":823,"y":1180,"width":623,"height":104},
		{"type":"link","url":"https://mattboegner.com/content/images/2023/01/Screen-Shot-2023-01-31-at-12.44.17-AM.png","id":"ad87a323c34333a3","x":-1089,"y":2200,"width":1002,"height":329},
		{"type":"text","text":"## [What are embeddings?](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings)\n\nOpenAI’s text embeddings measure the relatedness of text strings. Embeddings are commonly used for:\n\n-   **Search** (where results are ranked by relevance to a query string)\n-   **Clustering** (where text strings are grouped by similarity)\n-   **Recommendations** (where items with related text strings are recommended)\n-   **Anomaly detection** (where outliers with little relatedness are identified)\n-   **Diversity measurement** (where similarity distributions are analyzed)\n-   **Classification** (where text strings are classified by their most similar label)\n\nAn embedding is a vector (list) of floating point numbers. The [distance](https://platform.openai.com/docs/guides/embeddings/which-distance-function-should-i-use) between two vectors measures their relatedness. Small distances suggest high relatedness and large distances suggest low relatedness.","id":"86101792b5ec7ad8","x":-1129,"y":2800,"width":1082,"height":380},
		{"type":"link","url":"https://mattboegner.com/content/images/2023/01/vector-store-index.png","id":"58dbce5f258c98f8","x":250,"y":2137,"width":660,"height":455},
		{"type":"link","url":"https://docs.pinecone.io/docs/overview","id":"ef1557360913e4cb","x":380,"y":2720,"width":400,"height":714},
		{"type":"text","text":"Embedding Stores\n    - [facebookresearch/faiss: A library for efficient similarity search and clustering of dense vectors.](https://github.com/facebookresearch/faiss)\n    - [[Start Free, Scale Effortlessly.|Pinecone]] (OpenAI uses)\n    - [qdrant/qdrant: Qdrant - Vector Search Engine and Database for the next generation of AI applications. Also available in the cloud https://cloud.qdrant.io/ (github.com)](https://github.com/qdrant/qdrant)\n    - [jerryjliu/gpt_index: LlamaIndex (GPT Index) is a project that provides a central interface to connect your LLM's with external data. (github.com)](https://github.com/jerryjliu/gpt_index)\n\n[currentslab/awesome-vector-search: Collections of vector search related libraries, service and research papers (github.com)](https://github.com/currentslab/awesome-vector-search)\n","id":"acebcdef64b6eb9c","x":1040,"y":2211,"width":860,"height":289},
		{"type":"file","file":"resources/user_inquiry.png","id":"fc0cb418c2616253","x":-1380,"y":-746,"width":399,"height":226},
		{"type":"file","file":"resources/build_prompt.png","id":"87b93b83af299e1b","x":-980,"y":-834,"width":1402,"height":403},
		{"type":"file","file":"resources/build_prompt.png","id":"24ae6c6f5c240723","x":580,"y":1381,"width":1054,"height":303},
		{"type":"text","text":"# Top 1 Hottest AI Trends of 2023\n\n- ChatGPT\n    - Ask it about anything and get a reasonable-ish repsonse\n    - Excels at creative tasks, no exact answers\n    - Excels are parsing given information and summarizing/identifying patterns\n    - Not so hot at maths or specific information\n- Bing Chat\n    - Better than ChatGPT on a few dimensions\n    - Likely a newer/larger model, but that's not it\n    - Access to Bings Search Index\n    - Questions to underlying model get extra context from search\n    - Allows model to reason over a question AND relevant context all at once, leading to better answers\n- This is just the beginning\n    - Chat with your companies internal documents: [Getting Started — TPrompt](https://tprompt-docs.azurewebsites.net/gettingstarted/index.html)\n    - Ask different tools to answer different questions: [GPT+WolframAlpha+Whisper](https://huggingface.co/spaces/JavaFXpert/Chat-GPT-LangChain)","id":"37767ac0326596d7","x":-2844,"y":-600,"width":664,"height":600},
		{"type":"text","text":"## Limitations of using LLMs for Knowledge Retrieval\n- Using base model\n    - Limited to 4K (or 8K) chars for question and context\n    - Attempts to scale to more tokens (so far) significantly increase processing time in model\n- Fine-tuning base model\n    - Generating example prompt <> completion pairs is hard\n    - Gathering all the data which model needs to answer domain specific questions is also hard\n    - Domain data my also change regularly requiring very regular retraining (expensive)\n    - Current fine-tuning methods don't really teach models specifics fats very well, they can also quickly degrade the models ability to generalize","id":"144b536431930a72","x":-2844,"y":170,"width":664,"height":410},
		{"type":"text","text":"Generating embeddings involves:\n- Reading data and splitting into chunks\n    - up to size embeddings model can handle, typically ~1K chars\n- Feeding to embeddings model to get vectors\n- Storing vectors somewhere mapping to the data chunks so they can be searched","id":"44b8fdef15422b5c","x":-978,"y":3540,"width":780,"height":200},
		{"type":"file","file":"resources/gpt3_embedding.png","id":"32c7cd0c40280bb8","x":-100,"y":3380,"width":399,"height":211},
		{"type":"file","file":"resources/bert_embedding_space.png","id":"2341f509cfbeec67","x":51,"y":3693,"width":609,"height":363},
		{"type":"text","text":"## References\n- Amazing article: https://mattboegner.com/knowledge-retrieval-architecture-for-llms/\n","id":"bd92c8c31e46c008","x":-2800,"y":1290,"width":762,"height":149},
		{"type":"link","url":"https://twitter.com/goodside/status/1609972546954317824?s=20","id":"6f5c576517160567","x":-2778,"y":-1335,"width":532,"height":589}
	],
	"edges":[
		{"id":"b693a5f073ce3308","fromNode":"37767ac0326596d7","fromSide":"right","toNode":"2821f41f379d49a5","toSide":"left","label":"Bing Chat Style"},
		{"id":"fc4649609417ceb0","fromNode":"37767ac0326596d7","fromSide":"right","toNode":"12ec1dd5c1dd11aa","toSide":"left","label":"ChatGPT Style"},
		{"id":"ac945efeac3d900a","fromNode":"37767ac0326596d7","fromSide":"bottom","toNode":"144b536431930a72","toSide":"top","label":"Limitations of LLMs on their own"},
		{"id":"537e415511647cb8","fromNode":"2821f41f379d49a5","fromSide":"bottom","toNode":"930956982131d1e8","toSide":"top","label":"Retrieval"},
		{"id":"e7a215b09bd0896e","fromNode":"930956982131d1e8","fromSide":"bottom","toNode":"ad87a323c34333a3","toSide":"top","label":"Where do we look and how?"},
		{"id":"9c3b9fa1b96a9811","fromNode":"ad87a323c34333a3","fromSide":"bottom","toNode":"86101792b5ec7ad8","toSide":"top","label":"Embeddings"},
		{"id":"cfbf6e8b8f761077","fromNode":"ad87a323c34333a3","fromSide":"right","toNode":"58dbce5f258c98f8","toSide":"left","label":"Search"},
		{"id":"38e395a7e79aa6c1","fromNode":"86101792b5ec7ad8","fromSide":"bottom","toNode":"44b8fdef15422b5c","toSide":"top","label":"How to generate?"},
		{"id":"0a56e19c823de03e","fromNode":"37767ac0326596d7","fromSide":"top","toNode":"6f5c576517160567","toSide":"bottom","label":"lol"},
		{"id":"3737180ad244e739","fromNode":"930956982131d1e8","fromSide":"right","toNode":"24ae6c6f5c240723","toSide":"left","label":"Build Prompt, Get Answer"}
	]
}